{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchattacks\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.pytorch.org/vision/main/generated/torchvision.datasets.ImageNet.html\n",
    "# https://image-net.org/challenges/LSVRC/2012/2012-downloads.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load three different pretrained models from PyTorch. There are several pretrained models available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in .\\models\\pytorch_vision_main\n",
      "Using cache found in .\\models\\pytorch_vision_main\n",
      "Using cache found in .\\models\\pytorch_vision_main\n"
     ]
    }
   ],
   "source": [
    "torch.hub.set_dir(\".\\models\")\n",
    "alexnet  = torch.hub.load(\"pytorch/vision\", model=\"alexnet\",  weights=\"AlexNet_Weights.IMAGENET1K_V1\")\n",
    "resnet50 = torch.hub.load(\"pytorch/vision\", model=\"resnet50\", weights=\"ResNet50_Weights.IMAGENET1K_V1\")\n",
    "vit      = torch.hub.load(\"pytorch/vision\", model=\"vit_b_16\", weights=\"ViT_B_16_Weights.IMAGENET1K_V1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: In class we discussed a few adversarial attack techniques such as PGD, FGSM, etc.\n",
    "\n",
    "torchattacks is a framework which lets you attack any model with many such adversarial attack techniques. Install torchattacks and then use it to attack your loaded model with 3 different attack techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet = datasets.ImageNet(root = \".\\data\", split = 'eval', transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ]))\n",
    "dataset = datasets.CIFAR10(root = \".\\data\", train=False,  transform = transforms.Compose([\n",
    "    # transforms.Resize((229)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "]), download = True)\n",
    "images = []\n",
    "labels = []\n",
    "for x, y in dataset:\n",
    "    images.append(x)\n",
    "    labels.append(y)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m attk.set_normalization_used(mean=[\u001b[32m0.485\u001b[39m, \u001b[32m0.456\u001b[39m, \u001b[32m0.406\u001b[39m], std=[\u001b[32m0.229\u001b[39m, \u001b[32m0.224\u001b[39m, \u001b[32m0.225\u001b[39m])\n\u001b[32m     13\u001b[39m attk.set_mode_targeted_by_label(quiet=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m adv_images = \u001b[43mattk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m attk.save(adv_images, save_path=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\a\u001b[39;00m\u001b[33mdv_images\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdata_model\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_attk\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattk_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pt\u001b[39m\u001b[33m\"\u001b[39m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Github\\.venv\\Lib\\site-packages\\torchattacks\\attack.py:502\u001b[39m, in \u001b[36mAttack.__call__\u001b[39m\u001b[34m(self, inputs, labels, *args, **kwargs)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28mself\u001b[39m._change_model_mode(given_training)\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._normalization_applied \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minverse_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    503\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_normalization_applied(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    505\u001b[39m     adv_inputs = \u001b[38;5;28mself\u001b[39m.forward(inputs, labels, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Github\\.venv\\Lib\\site-packages\\torchattacks\\attack.py:124\u001b[39m, in \u001b[36mAttack.inverse_normalize\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    122\u001b[39m mean = \u001b[38;5;28mself\u001b[39m.normalization_used[\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m].to(inputs.device)\n\u001b[32m    123\u001b[39m std = \u001b[38;5;28mself\u001b[39m.normalization_used[\u001b[33m\"\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m\"\u001b[39m].to(inputs.device)\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m + mean\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "# https://adversarial-attacks-pytorch.readthedocs.io/en/latest/index.html\n",
    "# https://github.com/Harry24k/adversarial-attacks-pytorch\n",
    "# target_labels = [579 for _ in range(len(dataset.classes))]\n",
    "target_labels = [0 for _ in range(len(dataset.classes))]\n",
    "\n",
    "for model_idx, model in enumerate([resnet50, alexnet, vit]):\n",
    "    attk_PGD  = torchattacks.PGD (model, eps=8/255, alpha=1/255, steps=10, random_start=True)\n",
    "    attk_FGSM = torchattacks.FGSM(model, eps=8/255)\n",
    "    attk_CW   = torchattacks.CW  (model, c=1, kappa=0, steps=50, lr=0.01)\n",
    "\n",
    "    for attk_idx, attk in enumerate([attk_PGD, attk_FGSM, attk_CW]):\n",
    "        attk.set_normalization_used(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        attk.set_mode_targeted_by_label(quiet=True)\n",
    "        adv_images = attk(images, target_labels)\n",
    "        attk.save(adv_images, save_path=f\".\\adv_images\\data_model{model_idx}_attk{attk_idx}.pt\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Report accuracy in a 3Ã—3 table (3 rows for models and 3 columns for attack techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Choose any image of your choice and show the corresponding adversarial images generated by each attack technique from Step 2. \n",
    "\n",
    "For each adversarial image, report the average mean-squared error between the original image and the adversariaal image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
