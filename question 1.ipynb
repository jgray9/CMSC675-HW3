{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a629e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2093c4",
   "metadata": {},
   "source": [
    "Part 1: Multi-head Attention In this part, you will implement the core attention mechanism used throughout the Transformer.\n",
    "\n",
    "The multi-head attention module projects the input into multiple query, key, and value subspaces, applies scaled dot-product attention in parallel across heads, and concatenates the results. Your implementation should support multiple heads and include appropriate linear projections and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7bfe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, proj_drop: float = 0.1, atten_drop: float = 0.1) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model divisible by num_heads.\"\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Linear layers to project input to q, k, v\n",
    "        self.W_q = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(num_heads)])\n",
    "        self.W_k = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(num_heads)])\n",
    "        self.W_v = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(num_heads)])\n",
    "\n",
    "        # Output linear layer\n",
    "        self.ffn = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout for attention weights\n",
    "        self.proj_drop = nn.Dropout(p=proj_drop)\n",
    "        self.atten_drop = nn.Dropout(p=atten_drop)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        outputs: list[torch.Tensor] = []\n",
    "        for h in range(self.num_heads):\n",
    "            # Project inputs to q, k, v\n",
    "            Q: torch.Tensor = self.W_q[h](self.proj_drop(query))\n",
    "            K: torch.Tensor = self.W_k[h](self.proj_drop(key))\n",
    "            V: torch.Tensor = self.W_v[h](self.proj_drop(value))\n",
    "\n",
    "            # Calculate scaled dot-product attention scores\n",
    "            # Apply mask (if provided)\n",
    "            # (Hint: use torch.matmul and scale by sqrt(d_k))\n",
    "            Z: torch.Tensor = Q.matmul(K.transpose(0, 1)) / math.sqrt(self.d_k)\n",
    "            if(mask): Z *= mask\n",
    "            Z = F.softmax(Z, dim=0).matmul(V)\n",
    "\n",
    "            # Attention dropout\n",
    "            Z = self.atten_drop(Z)\n",
    "            outputs.append(Z)\n",
    "\n",
    "        # Concatenate heads and apply final linear projection\n",
    "        output = torch.cat(outputs, dim=1)\n",
    "        return self.ffn(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbfcc4c",
   "metadata": {},
   "source": [
    "Part 2: Positional Encoding Positional Encoding is used to inject the position information of each token in the input sequence. It uses sine and cosine functions of different frequencies to generate the positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc11c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_seq_length: int) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Define your position embedding function\n",
    "        self.pe = nn.Parameter(data=torch.zeros(max_seq_length, d_model), requires_grad=False)\n",
    "        for i in range(max_seq_length):\n",
    "            for pos in range(0, d_model, 2):\n",
    "                self.pe[i][pos] = math.sin(pos / math.pow(10000.0, 2*i / d_model))\n",
    "            for pos in range(1, d_model, 2):\n",
    "                self.pe[i][pos] = math.cos(pos / math.pow(10000.0, 2*i / d_model))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Add positional encoding to input x\n",
    "        x_seq_length = x.shape[0]\n",
    "        return x + self.pe[:x_seq_length,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397cdec",
   "metadata": {},
   "source": [
    "Part 3: EncoderLayer In this part, you will implement a single layer of the Transformer encoder, as shown in Figure 1.\n",
    "\n",
    "Each encoder layer consists of a multi-head self-attention block followed by a position-wise feed-forward network. Residual connections, layer normalization, and dropout are applied after each sub-layer. You should use your MultiHeadAttention module from Part 1 as a building block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c09c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1) -> None:\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        # Use your Multi-Head Attention module\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # Define position-wise feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model*4, d_model)\n",
    "        )\n",
    "\n",
    "        # Define two layer normalization layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Define dropout layers if needed\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        o  = self.norm1(self.attn(x,x,x) + x)\n",
    "        o  = self.dropout(o)\n",
    "        o2 = self.norm2(self.ffn(o)  + o)\n",
    "        return o2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd66b0c",
   "metadata": {},
   "source": [
    "Part 4: DecoderLayer Based on Figure 2, implement the DecoderLayer class, with two Multi-Head Attention layers, a Position-wise Feed-Forward layer, and three Layer Normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb70830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1) -> None:\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # Define projection layers for masked self-attention\n",
    "        self.masked_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Define projection layers for encoder-decoder attention\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # Define position-wise feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model*4, d_model)\n",
    "        )\n",
    "\n",
    "        # Define three layer normalization layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Optional: Define dropout layers if needed\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.dropout3 = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, enc_out: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # Masked multi-head self-attention on decoder input\n",
    "        o = self.norm1(self.masked_attn(x,x,x,mask) + x)\n",
    "        o = self.dropout1(o)\n",
    "\n",
    "        # Encoder-decoder attention (attend over encoder outputs)\n",
    "        o2 = self.norm2(self.attn(enc_out, enc_out, o) + o)\n",
    "        o2 = self.dropout2(o2)\n",
    "\n",
    "        # Position-wise feed-forward network\n",
    "        o3 = self.norm3(self.ffn(o2) + o2)\n",
    "        return o3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43372d2f",
   "metadata": {},
   "source": [
    "Part 5: Implement Transformer Class In this final part, you will assemble the full Transformer model using the components you have implemented in previous parts.\n",
    "\n",
    "Your model should include token embeddings, positional encodings, stacked encoder and decoder layers, and an output projection layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eec63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_heads: int, num_layers: int, dropout: float = 0.1, max_seq_length: int = 512):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Token embedding and Positional encoding\n",
    "        self.em = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.pe = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        # Encoder stack (N layers)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder stack (N layers)\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Output projection layer\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(d_model, vocab_size),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        enc_output = self.pe(self.em(src))\n",
    "        for layer in self.encoders:\n",
    "            enc_output = layer(enc_output)\n",
    "\n",
    "        dec_output = self.pe(self.em(tgt))\n",
    "        for layer in self.decoders:\n",
    "            dec_output = layer(dec_output, enc_output, mask)\n",
    "\n",
    "        return self.proj(dec_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
